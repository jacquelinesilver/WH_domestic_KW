# -*- coding: utf-8 -*-
"""org_quality.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cYd88ggPZWE7nEoFyVcLraXCMC1AgccI

Below, we import all the necesssary libraries for our analysis.
"""

#imports
!pip install ucimlrepo
!pip install requests beautifulsoup4
import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from urllib.parse import urljoin
from ucimlrepo import fetch_ucirepo, list_available_datasets
from sklearn.preprocessing import StandardScaler

"""Below, we scrape the wwhealthline affordable housing page to get a dictionary of organizations, with the key being the name of the organization and the value being the link to the organization specific page for each (to be used in analysis later on) and then repeat this to get the website of each organization."""

#snippits of suggested code from google-collab platform by not-anshuman/scraper-aditya
#function to get the names and urls of each organization mentioned on this site

#website with information about each housing organization in list
url = 'https://www.wwhealthline.ca/printServiceList.aspx?id=10865'

def get_organizations(url):

  response = requests.get(url)

  soup = BeautifulSoup(response.text, 'html.parser')
  #parse through web content and get each row of the table
  rows = soup.find_all("tr")

  organizations = {} #initialize the dictionary for the urls

  for row in rows:
      #get the name of the organization and the url for each
      link = row.find("td", id=lambda x: x and x.startswith("rptRegionalServices"))
      if link:
          link_tag = link.find("a")
          if link_tag:
              name = link_tag.get_text()
              org_url = urljoin(url, link_tag['href'])
              #add to dictionary of services
              organizations[name] = org_url
  return(organizations)

organizations = get_organizations(url)

#print the organizations and urls in a more comprehensive way
for key, value in organizations.items():
  print(f"{key}: {value}")

#function to get websites from each page of the database
def get_websites(url):

  response = requests.get(url)

  soup = BeautifulSoup(response.text, 'html.parser')


  link_tag = soup.find('a', id='ctl00_ContentPlaceHolder1_lnkUrl')

  #get the link to the website if it is specified on wwhealthline
  if link_tag:
      link = link_tag.get('href')
  else:
      link = 'No link found'
  return link

websites = {} #initialize the dictionary for the websites

for key, value in organizations.items():
  websites[key] = get_websites(value)
#get websites and print clearly

for key, value in websites.items():
  print(f"{key}: {value}")

"""Now that we have the name and resources for each organization, we can begin the analysis of which ones may be more suitable for the World Housing domestic project (in the Kitchener-Waterloo area specifically)"""

#see which general larger organizations have sub organizations (cover more than one specific area)
orgs = list(organizations.keys()) #get list of organizations

def find_larger_orgs(orgs):

  all_first2words = [] #intialize list for all first two words
  all_last2words = [] #intialize list for all first two words
  larger_orgs = {} #intialize dictionary for those whos first 2s get repeated

  for org in orgs:

    first_2_words =  ' '.join(org.split(' ')[:2])
    last_2_words =  ' '.join(org.split(' ')[2:])

    if first_2_words in all_first2words and first_2_words != '':
      if first_2_words in larger_orgs.keys():
        larger_orgs[first_2_words] += 1 #if it appears in the all list already, add another instance # to the larger orgs
      else:
        larger_orgs[first_2_words] = 2 #add it to larger list if not (2 because this is second time it appears)
    all_first2words.append(first_2_words) #add it to the list again (either way)

    #same as above with last two instead
    if last_2_words in all_last2words and last_2_words != '':
        if last_2_words in larger_orgs.keys():
          larger_orgs[last_2_words] += 1
        else:
          larger_orgs[last_2_words] = 2
    all_last2words.append(last_2_words)

  return larger_orgs

larger_orgs = find_larger_orgs(orgs)
print(larger_orgs)

plt.figure(figsize=(25, 10))
categories = larger_orgs.keys()
values = larger_orgs.values()
#plot results
plt.bar(categories, values)
plt.title('Wide Spread Housing Organizations')

plt.show()

"""In the above chart, we go through the data sources found through the wwhealthline site, which contains many affordable housing organizations, and observe which ones are under the same overarching company and may have a wider stretch of impact in the area."""

negative_indicators = ['temporary', 'exclusive', 'expensive', 'emergency']

positive_indicators = ['housing', 'sustainable', 'generational', 'welcoming', 'affordable', 'permanent', 'afford', 'family', 'women']

#function to get the description from the wwhealthline page for each organization
def get_description(url):

  response = requests.get(url)

  soup = BeautifulSoup(response.text, 'html.parser')

  description_td = soup.find('td', {'colspan': '3'})

  if description_td:
      description_text = description_td.get_text(separator=" ", strip=True)
  else:
      description_text = 'No description found'
  return (description_text)

descriptions = {} #initialize the dictionary for the websites

for key, value in organizations.items():
  descriptions[key] = get_description(value)
#get websites and print clearly

for key, value in descriptions.items():
  print(f"{key}: {value}")